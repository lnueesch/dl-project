{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Enable LaTeX font (optional)\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "def load_metrics(run_paths):\n",
    "    all_data = {}\n",
    "    for run_name, run_path in run_paths.items():\n",
    "        metrics_file = os.path.join(run_path, 'metrics.json')\n",
    "        if os.path.isfile(metrics_file):\n",
    "            with open(metrics_file, 'r') as f:\n",
    "                metrics = json.load(f)\n",
    "            all_data[run_name] = metrics\n",
    "    return all_data\n",
    "\n",
    "# Suppose you have paths like:\n",
    "run_path = './experiments/run_20250107_140754'\n",
    "run_paths = {\n",
    "    'config_1': run_path + '/config_1',\n",
    "    'config_2': run_path + '/config_2',\n",
    "    'config_3': run_path + '/config_3',\n",
    "    'config_4': run_path + '/config_4',\n",
    "    'config_5': run_path + '/config_5',\n",
    "}\n",
    "\n",
    "metrics_dict = load_metrics(run_paths)\n",
    "\n",
    "# Collect metrics into DataFrames\n",
    "dfs = {}\n",
    "for run_name, m in metrics_dict.items():\n",
    "    dfs[run_name] = pd.DataFrame(m)\n",
    "\n",
    "# The metrics we care about\n",
    "metrics_to_plot = ['nmi_true', 'ari_true', 'nmi_prev', 'silhouette', 'dbi']\n",
    "\n",
    "for metric in metrics_to_plot:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Plot this metric for each config/run\n",
    "    for run_name, df in dfs.items():\n",
    "        # Some metrics might have NaN/null (e.g. 'nmi_prev' for iteration 0), so drop them or fill\n",
    "        plt.plot(df[metric], label=run_name, marker='o')\n",
    "    \n",
    "    plt.title(f'{metric} across configurations')\n",
    "    plt.xlabel('Iteration')\n",
    "    if metric == 'dbi':\n",
    "        plt.ylabel(f'{metric} (lower is better)')\n",
    "    else:\n",
    "        plt.ylabel(f'{metric} (higher is better)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the best metrics as before\n",
    "best_metrics = {}\n",
    "\n",
    "for run_name, df in dfs.items():\n",
    "    best_nmi_true = df['nmi_true'].max()\n",
    "    best_ari_true = df['ari_true'].max()\n",
    "    best_silhouette = df['silhouette'].max()\n",
    "    best_dbi = df['dbi'].min()  # lower is better\n",
    "    \n",
    "    best_metrics[run_name] = {\n",
    "        'best_nmi_true': best_nmi_true,\n",
    "        'best_ari_true': best_ari_true,\n",
    "        'best_silhouette': best_silhouette,\n",
    "        'best_dbi': best_dbi\n",
    "    }\n",
    "\n",
    "best_metrics_df = pd.DataFrame(best_metrics).T\n",
    "print(best_metrics_df)\n",
    "\n",
    "# Now plot this as a table in a figure\n",
    "fig, ax = plt.subplots(figsize=(6, 2))  # adjust size as needed\n",
    "ax.axis('off')  # no x or y axes\n",
    "\n",
    "# Round the values for nicer display\n",
    "table_vals = best_metrics_df.round(3).values\n",
    "\n",
    "# Create the table\n",
    "table = ax.table(\n",
    "    cellText=table_vals,\n",
    "    colLabels=best_metrics_df.columns,\n",
    "    rowLabels=best_metrics_df.index,\n",
    "    loc='center'\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)  # enlarge the table if needed\n",
    "\n",
    "plt.title('Best Metrics per Configuration', y=1.08)  # move the title up\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_nmi_true_values = best_metrics_df['best_nmi_true']\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "best_nmi_true_values.plot(kind='bar', rot=0)\n",
    "plt.title('Best NMI vs GT across configurations')\n",
    "plt.xlabel('Configuration')\n",
    "plt.ylabel('Best NMI (higher is better)')\n",
    "plt.ylim(0, 1)  # if you know the range is [0,1]\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppose we want to compare these metrics for each config\n",
    "metrics_for_radar = ['best_nmi_true', 'best_ari_true', 'best_silhouette', 'best_dbi']\n",
    "\n",
    "# Normalize or invert DBI so that \"higher is better\" for consistent plotting, e.g. 1 / DBI\n",
    "radar_df = best_metrics_df.copy()\n",
    "radar_df['best_dbi'] = 1 / radar_df['best_dbi']  # invert DBI\n",
    "\n",
    "# Convert DataFrame to array\n",
    "values = radar_df[metrics_for_radar].values\n",
    "configs = radar_df.index\n",
    "\n",
    "# Angles around the circle\n",
    "num_metrics = len(metrics_for_radar)\n",
    "angles = np.linspace(0, 2*np.pi, num_metrics, endpoint=False)\n",
    "\n",
    "# Create subplots\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111, polar=True)\n",
    "\n",
    "for i, config in enumerate(configs):\n",
    "    # each row in values corresponds to a single config\n",
    "    stats = values[i]\n",
    "    stats = np.concatenate((stats, [stats[0]]))  # repeat the first value to close the polygon\n",
    "    angle_ext = np.concatenate((angles, [angles[0]]))\n",
    "\n",
    "    ax.plot(angle_ext, stats, label=config, marker='o')\n",
    "    ax.fill(angle_ext, stats, alpha=0.1)\n",
    "\n",
    "ax.set_xticks(angles)\n",
    "ax.set_xticklabels(metrics_for_radar)\n",
    "ax.set_title('Radar Chart of Best Metrics')\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
