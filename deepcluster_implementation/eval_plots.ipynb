{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Enable LaTeX font\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "def load_metrics(run_paths):\n",
    "    all_data = {}\n",
    "    for run_name, run_path in run_paths.items():\n",
    "        metrics_file = os.path.join(run_path, 'metrics.json')\n",
    "        if os.path.isfile(metrics_file):\n",
    "            with open(metrics_file, 'r') as f:\n",
    "                metrics = json.load(f)\n",
    "            all_data[run_name] = metrics\n",
    "    return all_data\n",
    "\n",
    "# Example usage:\n",
    "# Suppose you have { 'config_1': './experiments/run_20231001_123455/config_1', ... }\n",
    "run_paths = {\n",
    "    'config_1': './experiments/run_20250107_140754/config_1',\n",
    "    'config_2': './experiments/run_20250107_140754/config_2',\n",
    "}\n",
    "\n",
    "metrics_dict = load_metrics(run_paths)\n",
    "\n",
    "# Convert each run's metrics into a DataFrame and plot them\n",
    "for run_name, m in metrics_dict.items():\n",
    "    df = pd.DataFrame(m)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    axes[0].plot(df['nmi_true'], label='NMI vs GT')\n",
    "    axes[0].plot(df['ari_true'], label='ARI vs GT')\n",
    "    axes[0].plot(df['nmi_prev'], label='NMI vs Prev')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_title(f'{run_name} Clustering Metrics')\n",
    "\n",
    "    axes[1].plot(df['silhouette'], label='Silhouette')\n",
    "    axes[1].plot(df['dbi'], label='DBI')\n",
    "    axes[1].legend()\n",
    "    axes[1].set_title(f'{run_name} Cluster Quality')\n",
    "\n",
    "    plt.suptitle(f'Metrics for {run_name}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metrics = {}\n",
    "\n",
    "for run_name, m in metrics_dict.items():\n",
    "    df = pd.DataFrame(m)\n",
    "    best_nmi_true = df['nmi_true'].max()\n",
    "    best_ari_true = df['ari_true'].max()\n",
    "    best_silhouette = df['silhouette'].max()\n",
    "    best_dbi = df['dbi'].min()  # Assuming lower DBI is better\n",
    "\n",
    "    best_metrics[run_name] = {\n",
    "        'best_nmi_true': best_nmi_true,\n",
    "        'best_ari_true': best_ari_true,\n",
    "        'best_silhouette': best_silhouette,\n",
    "        'best_dbi': best_dbi\n",
    "    }\n",
    "\n",
    "best_metrics_df = pd.DataFrame(best_metrics).T\n",
    "print(best_metrics_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
